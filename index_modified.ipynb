{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataCo Supply Chain Analysis and Model Training\n",
        "This notebook performs exploratory data analysis (EDA), data cleaning, feature engineering, visualisation, and trains a classification model to predict `late_delivery_risk`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Initialise Spark session\n",
        "spark = (SparkSession.builder\n",
        "    .appName('SupplySession')\n",
        "    .master('local[*]')\n",
        "    .config('spark.sql.shuffle.partitions', '8')\n",
        "    .getOrCreate())\n",
        "\n",
        "# Load dataset\n",
        "csv_path = './DataCoSupplyChainDataset.csv'\n",
        "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
        "df.show(5, truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial EDA\n",
        "* Schema inspection\n",
        "* Row / column counts\n",
        "* Missing value overview\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('Rows:', df.count())\n",
        "print('Columns:', len(df.columns))\n",
        "df.printSchema()\n",
        "\n",
        "# Missing values per column\n",
        "def missing_counts(df):\n",
        "    exprs = []\n",
        "    for c in df.columns:\n",
        "        expr = F.count(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == ''), c).alias(c))\n",
        "        exprs.append(expr)\n",
        "    return df.select(exprs)\n",
        "missing_df = missing_counts(df)\n",
        "missing_df.show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualisation \u2013 Histograms and Heatmap (before cleaning)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sample a fraction for quick pandas visualisation\n",
        "sample_frac = 0.02 if df.count() > 100000 else 1.0\n",
        "pdf = df.sample(withReplacement=False, fraction=sample_frac).toPandas()\n",
        "\n",
        "# Histogram of delivery days (if column exists)\n",
        "if 'days_for_shipping_real' in pdf.columns:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.histplot(pdf['days_for_shipping_real'].dropna(), bins=30, kde=True)\n",
        "    plt.title('Distribution of Days for Shipping (raw)')\n",
        "    plt.xlabel('Days')\n",
        "    plt.show()\n",
        "\n",
        "# Correlation heatmap for numeric columns\n",
        "numeric_cols = pdf.select_dtypes(include='number').columns\n",
        "corr = pdf[numeric_cols].corr()\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(corr, cmap='coolwarm', center=0, annot=False)\n",
        "plt.title('Correlation Heatmap (raw)')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning \u2013 Drop high\u2011missing columns and impute\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Columns with >70% missing \u2013 drop them\n",
        "threshold = 0.7\n",
        "total_rows = df.count()\n",
        "missing_counts = missing_df.first()\n",
        "cols_to_drop = [c for c in df.columns if (missing_counts[c] / total_rows) > threshold]\n",
        "print('Dropping columns due to high missingness:', cols_to_drop)\n",
        "df = df.drop(*cols_to_drop)\n",
        "\n",
        "# Impute numeric columns with median, string columns with 'unknown'\n",
        "numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, (\n",
        "    __import__('pyspark.sql.types').IntegerType, __import__('pyspark.sql.types').LongType,\n",
        "    __import__('pyspark.sql.types').FloatType, __import__('pyspark.sql.types').DoubleType))]\n",
        "string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, __import__('pyspark.sql.types').StringType)]\n",
        "\n",
        "print(f'Numeric columns to impute: {numeric_cols}')\n",
        "for col in numeric_cols:\n",
        "    try:\n",
        "        quantiles = df.approxQuantile(col, [0.5], 0.01)\n",
        "        if quantiles:\n",
        "            median_val = quantiles[0]\n",
        "            df = df.na.fill({col: median_val})\n",
        "    except Exception as e:\n",
        "        print(f'Error imputing column {col}: {e}')\n",
        "\n",
        "for col in string_cols:\n",
        "    df = df.na.fill({col: 'unknown'})\n",
        "\n",
        "df.show(5, truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering \u2013 Distance and Aggregations\n",
        "We create a geographic distance feature (using haversine) between `origin_zipcode` and `destination_zipcode` (if available). We also aggregate per\u2011region statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    # convert decimal degrees to radians\n",
        "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
        "    c = 2 * math.asin(math.sqrt(a))\n",
        "    r = 6371  # Earth radius in km\n",
        "    return c * r\n",
        "\n",
        "# Assume we have latitude/longitude columns for origin and destination (example names)\n",
        "# If not present, this step will be skipped gracefully\n",
        "origin_lat = 'Latitude'\n",
        "origin_lon = 'Longitude'\n",
        "dest_lat = 'Latitude'\n",
        "dest_lon = 'Longitude'\n",
        "# Note: The dataset might not have separate origin/dest lat/lon. \n",
        "# Checking columns: 'Latitude', 'Longitude' are usually for the customer or store.\n",
        "# For now, let's just use what is available or skip if not found.\n",
        "# The original code used placeholders. I will use 'Latitude' and 'Longitude' if they exist, \n",
        "# but for distance we need two points. \n",
        "# If the dataset doesn't have origin/dest, we might skip this.\n",
        "# Let's stick to the original logic but with correct syntax.\n",
        "origin_lat = 'origin_latitude'\n",
        "origin_lon = 'origin_longitude'\n",
        "dest_lat = 'dest_latitude'\n",
        "dest_lon = 'dest_longitude'\n",
        "if set([origin_lat, origin_lon, dest_lat, dest_lon]).issubset(set(df.columns)):\n",
        "    haversine_udf = F.udf(haversine, __import__('pyspark.sql.types').DoubleType())\n",
        "    df = df.withColumn('geo_distance_km', haversine_udf(F.col(origin_lat), F.col(origin_lon), F.col(dest_lat), F.col(dest_lon)))\n",
        "    print('Added geo_distance_km feature')\n",
        "else:\n",
        "    print('Latitude/longitude columns not found \u2013 skipping distance feature')\n",
        "\n",
        "# Region\u2011level aggregations (example using 'Order Region')\n",
        "# The column name in the CSV is likely 'Order Region' (with space) or 'order_region'. \n",
        "# Spark infers schema with original names. Let's check columns.\n",
        "# But for now, fixing syntax is priority.\n",
        "target_region_col = 'Order Region' if 'Order Region' in df.columns else 'order_region'\n",
        "if target_region_col in df.columns:\n",
        "    region_stats = (df.groupBy(target_region_col)\n",
        "        .agg(\n",
        "            F.count('*').alias('region_order_count'),\n",
        "            F.avg('Sales').alias('region_avg_sales') if 'Sales' in df.columns else F.avg('sales').alias('region_avg_sales'),\n",
        "            F.avg('Days for shipping (real)').alias('region_avg_delivery_days') if 'Days for shipping (real)' in df.columns else F.avg('delivery_days').alias('region_avg_delivery_days')\n",
        "        ))\n",
        "    df = df.join(region_stats, on=target_region_col, how='left')\n",
        "    print('Joined region aggregation features')\n",
        "\n",
        "# Keep a concise set of features \u2013 we will select the most important later\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select Top Features (6\u20117) for Model Training\n",
        "We manually pick a handful of informative columns. Adjust as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "selected_features = [\n",
        "    'geo_distance_km',\n",
        "    'order_region',\n",
        "    'shipping_mode',\n",
        "    'customer_segment',\n",
        "    'sales',\n",
        "    'delivery_days',\n",
        "    'num_orders'  # from earlier aggregation if present\n",
        "]\n",
        "# Ensure columns exist \u2013 filter list\n",
        "selected_features = [c for c in selected_features if c in df.columns]\n",
        "print('Using features:', selected_features)\n",
        "\n",
        "# Assemble features for ML\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
        "\n",
        "# Identify categorical vs numeric\n",
        "categorical_cols = [c for c in selected_features if df.schema[c].dataType.simpleString() == 'string']\n",
        "numeric_cols = [c for c in selected_features if c not in categorical_cols]\n",
        "\n",
        "indexers = [StringIndexer(inputCol=c, outputCol=c+'_idx', handleInvalid='keep') for c in categorical_cols]\n",
        "encoders = [OneHotEncoder(inputCol=c+'_idx', outputCol=c+'_ohe') for c in categorical_cols]\n",
        "assembler = VectorAssembler(inputCols=[c+'_ohe' for c in categorical_cols] + numeric_cols, outputCol='features_unscaled')\n",
        "scaler = StandardScaler(inputCol='features_unscaled', outputCol='features')\n",
        "\n",
        "# Target column \u2013 ensure it exists\n",
        "target_col = 'late_delivery_risk'\n",
        "if target_col not in df.columns:\n",
        "    raise ValueError('Target column missing: ' + target_col)\n",
        "\n",
        "# Split data\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol=target_col, featuresCol='features', numTrees=100)\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, rf])\n",
        "model = pipeline.fit(train_df)\n",
        "preds = model.transform(test_df)\n",
        "\n",
        "# Evaluation\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "auc = BinaryClassificationEvaluator(labelCol=target_col, rawPredictionCol='probability').evaluate(preds)\n",
        "acc = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol='prediction', metricName='accuracy').evaluate(preds)\n",
        "print(f'ROC AUC: {auc:.4f}, Accuracy: {acc:.4f}')\n",
        "\n",
        "# Save model\n",
        "model.write().overwrite().save('./pipeline_model_selected')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualisation \u2013 After Cleaning / Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Re\u2011sample for quick plots after cleaning\n",
        "pdf_clean = df.sample(withReplacement=False, fraction=sample_frac).toPandas()\n",
        "\n",
        "# Histogram of selected numeric feature 'sales'\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.histplot(pdf_clean['sales'].dropna(), bins=30, kde=True)\n",
        "plt.title('Sales Distribution (cleaned)')\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap for selected numeric columns\n",
        "num_cols = [c for c in selected_features if pdf_clean[c].dtype != 'object']\n",
        "corr_clean = pdf_clean[num_cols].corr()\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr_clean, cmap='viridis', annot=True)\n",
        "plt.title('Correlation Heatmap (selected features)')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}